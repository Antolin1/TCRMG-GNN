{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python/')\n",
    "from json2graph import jsonFile2graph\n",
    "import networkx as nx\n",
    "from vocabulary import Vocabulary\n",
    "from graphUtils import plot_graph, graph2data, data2graph\n",
    "from neuralmodel import GNN_MoRec\n",
    "\n",
    "# generator must belong to {VIATRA,RANDOMEMF,ALLOY,RAND}\n",
    "generator = 'VIATRA'\n",
    "# modelType must belong to {Ecore,RDS,Yakindu}\n",
    "modelType = 'Yakindu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the vocabularies. These objects contains a dictionary that associates each node type or edge type to an integer between $[0\\dots n-1]$ where $n$ is the length of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_edges = Vocabulary()\n",
    "vocab_nodes = Vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a graph and adds the inverse edges (see background section in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOpposite(G):\n",
    "    if (modelType == 'Ecore'):\n",
    "        to_add = []\n",
    "        for n1,n2,e in G.edges:\n",
    "            typee = G[n1][n2][e]['type']\n",
    "            if (typee == 'eSuperTypes' or\n",
    "               typee == 'eType' or typee == 'eOpposite'):\n",
    "                to_add.append((n2,n1,typee+'_inv'))\n",
    "        for n1,n2,t in to_add:\n",
    "            G.add_edge(n1,n2,type=t)\n",
    "        return G\n",
    "    elif (modelType == 'RDS'):\n",
    "        to_add = []\n",
    "        for n1,n2,e in G.edges:\n",
    "            typee = G[n1][n2][e]['type']\n",
    "            if (typee == 'elements' or\n",
    "               typee == 'columns' or typee == 'indexes'\n",
    "               or typee == 'column' or typee == 'indexColumns'):\n",
    "                to_add.append((n2,n1,typee+'_inv'))\n",
    "        for n1,n2,t in to_add:\n",
    "            G.add_edge(n1,n2,type=t)\n",
    "        return G\n",
    "    elif (modelType == 'Yakindu'):\n",
    "        to_add = []\n",
    "        for n1,n2,e in G.edges:\n",
    "            typee = G[n1][n2][e]['type']\n",
    "            if (typee == 'vertices' or\n",
    "               typee == 'regions'):\n",
    "                to_add.append((n2,n1,typee+'_inv'))\n",
    "        for n1,n2,t in to_add:\n",
    "            G.add_edge(n1,n2,type=t)\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the models (graphs) generated by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def passFilter(G):\n",
    "    return len(G) >= 4\n",
    "\n",
    "files = glob.glob(\"../syntheticGraphs/\"+generator+\"/\"+modelType+\"/*.json\")\n",
    "mine = []\n",
    "for f in files:\n",
    "    try:\n",
    "        G = jsonFile2graph(f)\n",
    "        G = addOpposite(G)\n",
    "        if not passFilter(G):\n",
    "            continue\n",
    "        data = graph2data(G,0,vocab_nodes,vocab_edges)\n",
    "        mine.append(data)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the real models (graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../realGraphs/\"+modelType+\"/R1/*.json\")\n",
    "real = []\n",
    "for f in files:\n",
    "    G = jsonFile2graph(f)\n",
    "    G = addOpposite(G)\n",
    "    data = graph2data(G,1,vocab_nodes,vocab_edges)\n",
    "    real.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling the dataset to obtain a balanced dataset i.e., $50/50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "if len(mine) > len(real):\n",
    "    mine = random.sample(mine,len(real))\n",
    "elif len(mine) < len(real):\n",
    "    real = random.sample(real,len(mine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the real and generated graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(3)\n",
    "dataset = mine + real\n",
    "random.shuffle(dataset)\n",
    "print('Len train:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the set into train test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "train_len = int(0.6*len(dataset))\n",
    "val_len = int(0.15*len(dataset))\n",
    "test_len = len(dataset) - int(0.6*len(dataset)) - int(0.15*len(dataset))\n",
    "train, val, test = random_split(dataset, [train_len, val_len ,test_len], \n",
    "                                generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating loaders to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "train_loader = DataLoader(train, batch_size=32, num_workers = 5, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=1, num_workers = 5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the model using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader):\n",
    "    model.eval()\n",
    "    count = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            pred = model(data.x.cuda(), data.edge_index.cuda(),\n",
    "          torch.squeeze(data.edge_attr.cuda(),dim=1),data.batch.cuda())\n",
    "            if pred[0].item() > 0.5:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "            if pred == data.y.long().item():\n",
    "                count = count + 1\n",
    "    return count/len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nnUtils import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "path_to_model = './trainedModels/'+modelType+'-'+generator+'-GNN'\n",
    "model = GNN_MoRec(64,64,0.0,vocab_nodes,vocab_edges).cuda()\n",
    "\n",
    "epochs = 200\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "es = EarlyStopping(opt, model, path_to_model,mode='max',patience=50)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    b = 1\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        pred = model(data.x.cuda(), data.edge_index.cuda(),\n",
    "          torch.squeeze(data.edge_attr.cuda(),dim=1),data.batch.cuda())\n",
    "        \n",
    "        loss = criterion(torch.squeeze(pred), data.y.float().cuda())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        b = b + 1\n",
    "        \n",
    "    val_acc = evaluation(model, val_loader)\n",
    "    print('Epoch',e,'Loss',total_loss/b)\n",
    "    print('Eval',val_acc)\n",
    "    \n",
    "    if es.step(val_acc,e):\n",
    "        break\n",
    "        \n",
    "#Resultant model\n",
    "\n",
    "model2 = None\n",
    "try:\n",
    "    model2 = GNN_MoRec(64,64,0.0,vocab_nodes,vocab_edges).cuda()\n",
    "    checkpoint = torch.load(path_to_model)\n",
    "    model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model2.eval()\n",
    "except:\n",
    "    print('Saving model')\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict()\n",
    "            }, path_to_model)\n",
    "    model2 = GNN_MoRec(64,64,0.0,vocab_nodes,vocab_edges).cuda()\n",
    "    checkpoint = torch.load(path_to_model)\n",
    "    model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing C2ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model over the test set and reporting the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test, batch_size=1, num_workers = 5, shuffle=True)\n",
    "\n",
    "model2.eval()\n",
    "count = 0\n",
    "i0 = 0\n",
    "i1 = 0\n",
    "for data in test_loader:\n",
    "    \n",
    "    pred = model2(data.x.cuda(), data.edge_index.cuda(),\n",
    "          torch.squeeze(data.edge_attr,dim=1).cuda(),data.batch.cuda())\n",
    "    if pred[0].item() > 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    if pred == data.y.long().item():\n",
    "        count = count + 1\n",
    "    \n",
    "print('Acc', count/len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing C2ST using the accuracy and the length of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from C2ST import C2ST_pvalue\n",
    "\n",
    "acc =  count/len(test_loader)\n",
    "n_test = len(test_loader)\n",
    "print('p-value:', C2ST_pvalue(acc,n_test))\n",
    "print('samples', n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all graph in the test set that is synthetic and the model is sure that it is synthetic, the attention map is printed over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretation import heatMap, plot_graph_attention, importantSubgraph, getMapAttention\n",
    "i = 0\n",
    "for data in test:\n",
    "    G = data2graph(data,vocab_nodes,vocab_edges)\n",
    "    batch = torch.zeros(len(G)).long()\n",
    "    atts = model2.getAttentions(data.x.cuda(), data.edge_index.cuda(),\n",
    "          torch.squeeze(data.edge_attr.cuda(),dim=1),batch.cuda())\n",
    "    map_colors = getMapAttention(G,atts)\n",
    "    \n",
    "    pred = model2(data.x.cuda(), data.edge_index.cuda(),\n",
    "          torch.squeeze(data.edge_attr.cuda(),dim=1),batch.cuda())\n",
    "    if pred[0].item() < 0.1 and data.y.item() == 0:\n",
    "        plot_graph_attention(G,map_colors)\n",
    "        #plot_graph_attention(importantSubgraph(G, atts.detach().cpu().numpy(), 0.2, 2),map_colors)\n",
    "        #heatMap(G,atts,str(i),'./interpretation/'+modelType+'/'+generator+'/')\n",
    "        #heatMap(importantSubgraph(G, atts.detach().cpu().numpy(), 0.2, 2),atts,str(i),\n",
    "        #        './interpretation/'+modelType+'/'+generator+'/subgraph/')\n",
    "        i = i + 1\n",
    "        print('--'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
