{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "sys.path.append('../python/')\n",
    "from json2graph import jsonFile2graph\n",
    "from graphUtils import plot_graph\n",
    "from statsUtils import whichFitsBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading $R_{II}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "files = glob.glob(\"../realGraphs/RDS/R2/*.json\")\n",
    "\n",
    "Gs = []\n",
    "for file in files:\n",
    "    Gs.append(jsonFile2graph(file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random EMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each rule in RandomEMF, depending on the type of rule, we estimate its parameters. More concretely, for shapes we use the function `whichFitsBetter` that selects the best distribuntion by using maximum likeihood. For priorities in alternative rules, the procedure described in the paper is done and it is based on counting each different alternative in the set $R_{II}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of tables per database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "\n",
    "```\n",
    "Root : Database ->\n",
    "\t\telements += Tables(self)#Distribution(parameters);\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numberSubvertex = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if G.nodes[n]['type'] =='Database':\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                if (G.nodes[e]['type'] == 'Table'):\n",
    "                    cont = cont + 1\n",
    "            numberSubvertex.append(cont)\n",
    "            \n",
    "bins = np.arange(0, 100, 1)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(numberSubvertex)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import nbinom\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "t = np.arange(0, 50, 1)\n",
    "d = nbinom.pmf(t, 4.0, 0.435681, 0)\n",
    "#np.exp(-np.mean(numberClassifiers))*np.power(np.mean(numberClassifiers), t)/factorial(t)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)\n",
    "plt.plot(t, d, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns per table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "```\n",
    "Tables (Database d) : Table -> \n",
    "\t\tcolumns += Columns(d)#Distribution(parameters);\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numberSubvertex = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if G.nodes[n]['type'] =='Table':\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                if (G.nodes[e]['type'] == 'Column'):\n",
    "                    cont = cont + 1\n",
    "            numberSubvertex.append(cont)\n",
    "            \n",
    "bins = np.arange(0, 100, 1)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(numberSubvertex)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, 50, 1)\n",
    "d = nbinom.pmf(t, 3, 0.3630, 0)\n",
    "#np.exp(-np.mean(numberClassifiers))*np.power(np.mean(numberClassifiers), t)/factorial(t)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)\n",
    "plt.plot(t, d, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes per table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "```\n",
    "Tables (Database d) : Table -> \n",
    "\t\tindexes += Indexx(self)#Distribution(parameters);\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberSubvertex = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if G.nodes[n]['type'] =='Table':\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                if (G.nodes[e]['type'] == 'Index'):\n",
    "                    cont = cont + 1\n",
    "            numberSubvertex.append(cont)\n",
    "            \n",
    "bins = np.arange(0, 10, 1)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(numberSubvertex)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frefs per Colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "```\n",
    "Columns (Database d): Column -> \n",
    "\t\tforeignReferences += if (d.elements.filter[it instanceof Table].\n",
    "\t\t\tmap[it as Table].flatMap[it.columns].size >= 1\n",
    "\t\t) \n",
    "\t\tReferenceF(d, self)#Distribution(parameters)\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if (G.nodes[n]['type'] == 'Column'):\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                for e2 in G[n][e]:\n",
    "                     if (G[n][e][e2]['type'] == 'foreignReferences'):\n",
    "                        cont = cont + 1\n",
    "            number.append(cont)\n",
    "bins = np.arange(0, 10, 1)\n",
    "plt.hist(number, bins = bins, alpha=0.5, density = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(number)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefs per column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "```\n",
    "Columns (Database d): Column -> \n",
    "\t\tprimaryReferences += if (d.elements.filter[it instanceof Table].\n",
    "\t\t\tmap[it as Table].flatMap[it.columns].size >= 1\n",
    "\t\t) ReferenceP(d, self)#Distribution(parameters)\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if (G.nodes[n]['type'] == 'Column'):\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                for e2 in G[n][e]:\n",
    "                     if (G[n][e][e2]['type'] == 'primaryReferences'):\n",
    "                        cont = cont + 1\n",
    "            number.append(cont)\n",
    "bins = np.arange(0, 10, 1)\n",
    "plt.hist(number, bins = bins, alpha=0.5, density = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(number)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rule:\n",
    "```\n",
    "Indexx (Table t) : Index->\n",
    "\t\tindexColumns += IndexColumnss(t)#Distribution(parameters)\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberSubvertex = []\n",
    "for G in Gs:\n",
    "    for n in G:\n",
    "        if G.nodes[n]['type'] =='Index':\n",
    "            cont = 0\n",
    "            for e in G[n]:\n",
    "                if (G.nodes[e]['type'] == 'IndexColumn'):\n",
    "                    cont = cont + 1\n",
    "            numberSubvertex.append(cont)\n",
    "            \n",
    "bins = np.arange(0, 10, 1)\n",
    "plt.hist(numberSubvertex, bins = bins, alpha=0.5, density = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = whichFitsBetter(numberSubvertex)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIATRA and ALLOY, estimating the scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For VIATRA and ALLOY, the distribution over the objects (i.e., $P(o)$) needs to be approximated. First, we calculate $\\{o_1,\\dots,o_n\\}$ by counting the number of objects of each model in $R_{II}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the KDE function:\n",
    "\n",
    "$$\\hat{f}_{h,K}(o)=\\frac{1}{nh}\\sum_{i=1}^nK\\left(\\frac{o-o_i}{h}\\right).$$\n",
    "\n",
    "Where $K \\in \\{\\text{gaussian, tophat}\\}$ and $h\\in \\texttt{np.logspace(-2, -1, 20)}$. $K$ and $h$ are fixed using crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "numberObjects = [[len([n for n in G])] for G in Gs]\n",
    "\n",
    "params = {'bandwidth': np.logspace(-2, -1, 20),\n",
    "         'kernel':['gaussian', 'tophat']}\n",
    "grid = GridSearchCV(KernelDensity(), params, n_jobs = 8)\n",
    "grid.fit(np.array(numberObjects))\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "print(\"best kernel: {0}\".format(grid.best_estimator_.kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the histogram samples are close to the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kde = grid.best_estimator_\n",
    "new_data = kde.sample(1000, random_state=0)\n",
    "new_data = new_data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "size_bin=2\n",
    "bins = np.arange(-50, 200, size_bin)\n",
    "numberObjects = [len([n for n in G]) for G in Gs]\n",
    "hist = plt.hist(numberObjects, bins = bins, alpha=0.5,density=True)\n",
    "plt.hist(new_data, bins = bins, alpha=0.5,density=True)\n",
    "probs = hist[0]\n",
    "probs = (probs/np.sum(probs))\n",
    "objs = hist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate the config files for VIATRA and ALLOY. These files are already provided together with the final and generated models. Therefore, you should not execute these code snippets.\n",
    "\n",
    "**Note**: For Alloy we generate more samples (1000) since the generator fails more when it looks for a solution to the logic problem. Also we filter samples that are greater than 20 since ALLOY is not scalable using this meta-model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import random\n",
    "i = 0\n",
    "new_data = kde.sample(300, random_state=0)\n",
    "new_data = new_data.reshape(-1)\n",
    "for s in new_data:\n",
    "    with open('../configurationFiles/RDS/model.vsconfig', 'r') as file:\n",
    "        data = file.read()\n",
    "    x = data.replace(\"#node += 20..21\", \"#node += \"+str(int(s)))\n",
    "    x = x.replace(\"debug =\\t\\t\\t\\\"outputs/debug\\\"\",\"debug =\\t\\t\\t\\\"outputs\"+str(i)+\"/debug\\\"\")\n",
    "    x = x.replace(\"log =\\t\\t\\t\\\"outputs/log.txt\\\"\",\"log =\\t\\t\\t\\\"outputs\"+str(i)+\"/log.txt\\\"\")\n",
    "    x = x.replace(\"output =\\t\\t\\\"outputs/models\\\"\",\"output =\\t\\t\\\"outputs\"+str(i)+\"/models\\\"\")\n",
    "    x = x.replace(\"runs = 10\",\"runs = 1\")\n",
    "    with open(\"../configurationFiles/RDS/VIATRA/rdsGen\"+str(i)+\".vsconfig\", \"w\") as text_file:\n",
    "        text_file.write(x)\n",
    "        i = i + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import random\n",
    "i = 0\n",
    "new_data = kde.sample(1000, random_state=0)\n",
    "new_data = new_data.reshape(-1)\n",
    "for s in new_data:\n",
    "    if int(s) > 20:\n",
    "        continue\n",
    "    with open('../configurationFiles/RDS/modelAlloy.vsconfig', 'r') as file:\n",
    "        data = file.read()\n",
    "    x = data.replace(\"#node += 20..21\", \"#node += \"+str(int(s)))\n",
    "    x = x.replace(\"debug =\\t\\t\\t\\\"outputs/debug\\\"\",\"debug =\\t\\t\\t\\\"outputs\"+str(i)+\"/debug\\\"\")\n",
    "    x = x.replace(\"log =\\t\\t\\t\\\"outputs/log.txt\\\"\",\"log =\\t\\t\\t\\\"outputs\"+str(i)+\"/log.txt\\\"\")\n",
    "    x = x.replace(\"output =\\t\\t\\\"outputs/models\\\"\",\"output =\\t\\t\\\"outputs\"+str(i)+\"/models\\\"\")\n",
    "    x = x.replace(\"ViatraSolver\", \"AlloySolver\")\n",
    "    with open(\"../configurationFiles/RDS/ALLOY/rdsGen\"+str(i)+\".vsconfig\", \"w\") as text_file:\n",
    "        text_file.write(x)\n",
    "        i = i + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do thy same as the previous section but considering pairs $(o,d)$ where $o$ is the number of objects and $d$ is the average out degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "deg_objects = [np.mean([G.out_degree(n) for n in G]) for G in Gs]\n",
    "objects_deg = np.array(list(zip(numberObjects,deg_objects)))\n",
    "objects_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bandwidth': np.logspace(-2, -1, 20),\n",
    "         'kernel':['gaussian', 'tophat']}\n",
    "grid2 = GridSearchCV(KernelDensity(), params, n_jobs = 10)\n",
    "grid2.fit(objects_deg)\n",
    "print(\"best bandwidth: {0}\".format(grid2.best_estimator_.bandwidth))\n",
    "print(\"best kernel: {0}\".format(grid2.best_estimator_.kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde2 = grid2.best_estimator_\n",
    "new_data2 = kde2.sample(500, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the new data generated (i.e., new pairs $(o,d)$), we call the RANDOM generator in order to generate the models. Doing something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import random\n",
    "import subprocess\n",
    "i = 0\n",
    "for s in new_data2:\n",
    "    subprocess.call(['java', '-jar', '../randJar/instantiate.jar', \n",
    "                     '-m','path to metamodel',\n",
    "                    '-f','-n','1','-s',str(s[0]),'-d',str(s[1]),'-o',\n",
    "                     'path to output folder',\n",
    "                    '-e',str(i)])\n",
    "    i = i + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated models used to report the results in the paper are already provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
