{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python/')\n",
    "from json2graph import jsonFile2graph\n",
    "import networkx as nx\n",
    "from vocabulary import Vocabulary\n",
    "from graphUtils import plot_graph, graph2data, data2graph\n",
    "from neuralmodel import GNN_MoRec\n",
    "\n",
    "# generator must belong to {VIATRA,RANDOMEMF,ALLOY,RAND}\n",
    "generator = 'RAND'\n",
    "# modelType must belong to {Ecore,RDS,Yakindu}\n",
    "modelType = 'Ecore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading vocabularies and train/test/val used in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the vocabularies. These objects contains a dictionary that associates each node type or edge type to an integer between $[0\\dots n-1]$ where $n$ is the length of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vocab_edges = Vocabulary()\n",
    "vocab_nodes = Vocabulary()\n",
    "\n",
    "with open('../Vocabularies/vocab_edges-'+modelType+'-'+generator+'.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    vocab_edges.word2id_names = data\n",
    "    vocab_edges.id2word_names = {y:x for x,y in data.items()}\n",
    "    \n",
    "with open('../Vocabularies/vocab_nodes-'+modelType+'-'+generator+'.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    vocab_nodes.word2id_names = data\n",
    "    vocab_nodes.id2word_names = {y:x for x,y in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train/test and val splits used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train = torch.load('../TrainTestValSplits/Train-'+modelType+'-'+generator)\n",
    "val = torch.load('../TrainTestValSplits/Val-'+modelType+'-'+generator)\n",
    "test = torch.load('../TrainTestValSplits/Test-'+modelType+'-'+generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading that model was used to report the results in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GNN_MoRec(64,64,0.0,vocab_nodes,vocab_edges).cpu()\n",
    "path_to_model = '../models/'+modelType+'/'+modelType+'-'+generator+'-GNN'\n",
    "checkpoint = torch.load(path_to_model,map_location=torch.device('cpu'))\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing C2ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model over the test set and reporting the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test, batch_size=1, num_workers = 5, shuffle=True)\n",
    "\n",
    "model2.eval()\n",
    "count = 0\n",
    "i0 = 0\n",
    "i1 = 0\n",
    "for data in test_loader:\n",
    "    \n",
    "    pred = model2(data.x.cpu(), data.edge_index.cpu(),\n",
    "          torch.squeeze(data.edge_attr,dim=1).cpu(),data.batch.cpu())\n",
    "    if pred[0].item() > 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    if pred == data.y.long().item():\n",
    "        count = count + 1\n",
    "    \n",
    "print('Acc', count/len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing C2ST using the accuracy and the length of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from C2ST import C2ST_pvalue\n",
    "\n",
    "acc =  count/len(test_loader)\n",
    "n_test = len(test_loader)\n",
    "print('p-value:', C2ST_pvalue(acc,n_test))\n",
    "print('samples', n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all graph in the test set that is synthetic and the model is sure that it is synthetic, the attention map is printed over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretation import heatMap, plot_graph_attention, importantSubgraph, getMapAttention\n",
    "i = 0\n",
    "for data in test:\n",
    "    G = data2graph(data,vocab_nodes,vocab_edges)\n",
    "    batch = torch.zeros(len(G)).long()\n",
    "    atts = model2.getAttentions(data.x.cpu(), data.edge_index.cpu(),\n",
    "          torch.squeeze(data.edge_attr.cpu(),dim=1),batch.cpu())\n",
    "    map_colors = getMapAttention(G,atts)\n",
    "    \n",
    "    pred = model2(data.x.cpu(), data.edge_index.cpu(),\n",
    "          torch.squeeze(data.edge_attr.cpu(),dim=1),batch.cpu())\n",
    "    if pred[0].item() < 0.1 and data.y.item() == 0:\n",
    "        plot_graph_attention(G,map_colors)\n",
    "        #plot_graph_attention(importantSubgraph(G, atts.detach().cpu().numpy(), 0.2, 2),map_colors)\n",
    "        #heatMap(G,atts,str(i),'./interpretation/'+modelType+'/'+generator+'/')\n",
    "        #heatMap(importantSubgraph(G, atts.detach().cpu().numpy(), 0.2, 2),atts,str(i),\n",
    "        #        './interpretation/'+modelType+'/'+generator+'/subgraph/')\n",
    "        i = i + 1\n",
    "        print('--'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
